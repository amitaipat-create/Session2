{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amitaipat-create/Session2/blob/main/Mission_2_Practice_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agents with memory, conversation history, multi-turn conversations"
      ],
      "metadata": {
        "id": "GSZW6XkS2LC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install langchain==0.3.27 langchain-openai==0.3.29 langchain-community==0.3.27 pydantic==2.11.10 requests==2.32.4 --quiet"
      ],
      "metadata": {
        "id": "zArfFIO2EY4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables from .env file (for local development)\n",
        "load_dotenv()\n",
        "\n",
        "# OpenAI API Key\n",
        "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")"
      ],
      "metadata": {
        "id": "s8YQDMpBCz-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sD6w1WebCqcg"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.tools import tool\n",
        "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "try:\n",
        "    from IPython.display import display, Markdown\n",
        "except ImportError:\n",
        "    def display(x): print(x)\n",
        "    def Markdown(x): return x\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# SETUP: Create Simple Tools\n",
        "# ============================================================================\n",
        "\n",
        "@tool\n",
        "def get_weather(city: str) -> str:\n",
        "    \"\"\"Get weather information for a city\"\"\"\n",
        "    # Mock weather data (in real app, call weather API)\n",
        "    weather_data = {\n",
        "        \"paris\": \"Sunny, 22°C\",\n",
        "        \"london\": \"Cloudy, 15°C\",\n",
        "        \"tokyo\": \"Rainy, 18°C\",\n",
        "        \"new york\": \"Sunny, 25°C\"\n",
        "    }\n",
        "    city_lower = city.lower()\n",
        "    return weather_data.get(city_lower, f\"Weather data not available for {city}\")\n",
        "\n",
        "@tool\n",
        "def get_population(city: str) -> str:\n",
        "    \"\"\"Get population of a city\"\"\"\n",
        "    # Mock population data (in real app, call API)\n",
        "    population_data = {\n",
        "        \"paris\": \"2.1 million\",\n",
        "        \"london\": \"9.0 million\",\n",
        "        \"tokyo\": \"14.0 million\",\n",
        "        \"new york\": \"8.5 million\"\n",
        "    }\n",
        "    city_lower = city.lower()\n",
        "    return population_data.get(city_lower, f\"Population data not available for {city}\")\n",
        "\n",
        "tools = [get_weather, get_population]"
      ],
      "metadata": {
        "id": "u-2hnNB1C5S3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CONCEPT 1: Agent Without Memory (Loses Context)\n",
        "# ============================================================================\n",
        "\n",
        "# Agent without memory forgets previous conversations\n",
        "# Each question is treated independently\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful assistant. Use tools to answer questions.\"),\n",
        "    (\"human\", \"{input}\"),\n",
        "    (\"placeholder\", \"{agent_scratchpad}\"),\n",
        "])\n",
        "\n",
        "agent = create_tool_calling_agent(llm, tools, prompt)\n",
        "agent_executor_no_memory = AgentExecutor(agent=agent, tools=tools, verbose=False)\n",
        "\n",
        "# First question\n",
        "result1 = agent_executor_no_memory.invoke({\"input\": \"What's the weather in Paris?\"})\n",
        "print(\"Question 1: What's the weather in Paris?\")\n",
        "print(f\"Answer: {result1['output']}\")\n",
        "\n",
        "# Follow-up question (agent doesn't remember Paris was mentioned)\n",
        "result2 = agent_executor_no_memory.invoke({\"input\": \"What about the population?\"})\n",
        "print(\"\\nQuestion 2: What about the population?\")\n",
        "print(f\"Answer: {result2['output']}\")\n",
        "\n",
        "# Problem: Agent doesn't know \"the population\" refers to Paris!"
      ],
      "metadata": {
        "id": "hikKn5QBC8yY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CONCEPT 2: Agent With Memory (Remembers Context)\n",
        "# ============================================================================\n",
        "\n",
        "# Memory stores conversation history so agent remembers previous messages\n",
        "# This enables natural follow-up questions\n",
        "\n",
        "# Create memory to store conversation\n",
        "memory = ConversationBufferMemory(\n",
        "    memory_key=\"chat_history\",\n",
        "    return_messages=True\n",
        ")\n",
        "\n",
        "# Update prompt to include chat history\n",
        "prompt_with_memory = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful assistant. Use tools to answer questions. Remember previous conversation.\"),\n",
        "    (\"placeholder\", \"{chat_history}\"),  # Memory goes here\n",
        "    (\"human\", \"{input}\"),\n",
        "    (\"placeholder\", \"{agent_scratchpad}\"),\n",
        "])\n",
        "\n",
        "agent_with_memory = create_tool_calling_agent(llm, tools, prompt_with_memory)\n",
        "\n",
        "# Create executor with memory\n",
        "agent_executor = AgentExecutor(\n",
        "    agent=agent_with_memory,\n",
        "    tools=tools,\n",
        "    memory=memory,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# First question\n",
        "result1 = agent_executor.invoke({\"input\": \"What's the weather in Paris?\"})\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"WITH MEMORY:\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\nQuestion 1: What's the weather in Paris?\")\n",
        "print(f\"Answer: {result1['output']}\")\n",
        "\n",
        "# Follow-up question (agent remembers Paris from previous question!)\n",
        "result2 = agent_executor.invoke({\"input\": \"What about the population?\"})\n",
        "print(\"\\nQuestion 2: What about the population?\")\n",
        "print(f\"Answer: {result2['output']}\")\n",
        "\n",
        "# Success! Agent knows \"the population\" refers to Paris!"
      ],
      "metadata": {
        "id": "GcxfWVBxDBPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CONCEPT 3: Multi-Turn Conversation\n",
        "# ============================================================================\n",
        "\n",
        "# Reset memory for clean example\n",
        "memory2 = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "agent_executor2 = AgentExecutor(\n",
        "    agent=create_tool_calling_agent(llm, tools, prompt_with_memory),\n",
        "    tools=tools,\n",
        "    memory=memory2,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# Natural conversation flow\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"MULTI-TURN CONVERSATION:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Turn 1\n",
        "response1 = agent_executor2.invoke({\"input\": \"Tell me about London\"})\n",
        "print(\"\\nYou: Tell me about London\")\n",
        "print(f\"Agent: {response1['output']}\")\n",
        "\n",
        "# Turn 2 - Follow-up question\n",
        "response2 = agent_executor2.invoke({\"input\": \"What's the weather there?\"})\n",
        "print(\"\\nYou: What's the weather there?\")\n",
        "print(f\"Agent: {response2['output']}\")\n",
        "\n",
        "# Turn 3 - Another follow-up\n",
        "response3 = agent_executor2.invoke({\"input\": \"And how many people live there?\"})\n",
        "print(\"\\nYou: And how many people live there?\")\n",
        "print(f\"Agent: {response3['output']}\")"
      ],
      "metadata": {
        "id": "QxVFSasPDEkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Notice: Agent understands \"there\" refers to London from the conversation!\n",
        "\n",
        "# ============================================================================\n",
        "# EXERCISES\n",
        "# ============================================================================\n",
        "\n",
        "# EXERCISE 1: Try a conversation without memory\n",
        "# Create a new agent without memory and ask:\n",
        "# \"What's the weather in Tokyo?\" then \"What about the population?\"\n",
        "# See how it fails to understand the follow-up\n",
        "\n",
        "# EXERCISE 2: Try a conversation with memory\n",
        "# Create a new agent with memory and ask the same questions\n",
        "# See how it remembers context!\n",
        "\n",
        "# EXERCISE 3: Build a multi-turn conversation\n",
        "# Ask about a city, then ask follow-up questions using \"it\", \"there\", \"that city\"\n",
        "# Watch how the agent maintains context throughout the conversation\n",
        "\n",
        "display(Markdown(\"## ✓ Practice Complete!\"))\n",
        "display(Markdown(\"You learned: Agents with memory, conversation history, multi-turn conversations\"))"
      ],
      "metadata": {
        "id": "7w6b1YxODJD7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}